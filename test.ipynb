{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f973f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp312-cp312-macosx_14_0_arm64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: neo4j in /opt/anaconda3/lib/python3.12/site-packages (5.28.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (3.2.1)\n",
      "Collecting fastapi\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting uvicorn\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.11.7)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from python-docx) (4.12.2)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (1.99.6)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.10.1)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from neo4j) (2024.1)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi)\n",
      "  Using cached starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn) (8.1.7)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.1-cp312-cp312-macosx_15_0_arm64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Using cached langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Using cached langchain_openai-0.3.29-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp312-cp312-macosx_14_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Using cached langsmith-0.4.13-py3-none-any.whl (372 kB)\n",
      "Downloading orjson-3.11.1-cp312-cp312-macosx_15_0_arm64.whl (129 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.5/633.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, uvicorn, typing-inspect, rank-bm25, orjson, marshmallow, httpx-sse, faiss-cpu, starlette, dataclasses-json, langsmith, fastapi, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: zstandard\n",
      "\u001b[2K    Found existing installation: zstandard 0.22.0\n",
      "\u001b[2K    Uninstalling zstandard-0.22.0:\n",
      "\u001b[2K      Successfully uninstalled zstandard-0.22.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [langchain-community]ngchain-community]ters]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0.post1 fastapi-0.116.1 httpx-sse-0.4.1 langchain-0.3.27 langchain-community-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.29 langchain-text-splitters-0.3.9 langsmith-0.4.13 marshmallow-3.26.1 orjson-3.11.1 rank-bm25-0.2.2 starlette-0.47.2 typing-inspect-0.9.0 uvicorn-0.35.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx langchain langchain-openai langchain-community faiss-cpu neo4j networkx fastapi uvicorn python-dotenv pydantic tiktoken rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9759c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] DOCS_PATH = /Users/drashteeparmar/Drashtee/Drashtee Projects/AI_Powered_Policy_Compliance_System/policies\n"
     ]
    }
   ],
   "source": [
    "# %% Config\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os, pathlib, hashlib, json, time\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "DOCS_PATH = os.getenv(\"DOCS_PATH\", os.path.join(os.getcwd(), \"policies\"))\n",
    "assert os.path.isdir(DOCS_PATH), f\"Policies folder not found: {DOCS_PATH}\"\n",
    "print(f\"[CONFIG] DOCS_PATH = {DOCS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db5863c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEO4J] Connected: bolt://localhost:7687\n"
     ]
    }
   ],
   "source": [
    "# %% Neo4j connection (env-driven) + fallback\n",
    "import os, sys, networkx as nx\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")          # e.g., neo4j://localhost:7687 or neo4j+s://<auradb>\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "try:\n",
    "    from neo4j import GraphDatabase, basic_auth\n",
    "except Exception as e:\n",
    "    GraphDatabase = None\n",
    "\n",
    "class GraphStore:\n",
    "    def __init__(self, uri: Optional[str], user: Optional[str], pwd: Optional[str]):\n",
    "        self.enabled = False\n",
    "        self.driver = None\n",
    "        self.G = nx.MultiDiGraph()\n",
    "        if uri and GraphDatabase:\n",
    "            try:\n",
    "                self.driver = GraphDatabase.driver(uri, auth=basic_auth(user, pwd))\n",
    "                with self.driver.session() as s:\n",
    "                    s.run(\"RETURN 1\")\n",
    "                self.enabled = True\n",
    "                print(f\"[NEO4J] Connected: {uri}\")\n",
    "            except Exception as e:\n",
    "                print(\"[NEO4J] Connection failed, using in-memory graph:\", repr(e))\n",
    "        else:\n",
    "            print(\"[NEO4J] Driver not available or URI missing; using in-memory graph.\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "\n",
    "graph_store = GraphStore(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7555ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPENAI] Client initialized from .env\n"
     ]
    }
   ],
   "source": [
    "# %% Entities (LLM + regex fallback)\n",
    "import re, json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "oai = OpenAI()  # reads OPENAI_API_KEY from env\n",
    "print(\"[OPENAI] Client initialized from .env\")\n",
    "\n",
    "ENTITY_PROMPT = \"\"\"Extract key entities from the text. Return JSON:\n",
    "{\"entities\":[{\"name\":\"string\",\"type\":\"Policy|Procedure|Fee|TimeWindow|Reward|Other\"}]}.\n",
    "Text:\n",
    "```{text}```\"\"\"\n",
    "\n",
    "def llm_entities(text: str) -> List[Dict[str, Any]]:\n",
    "    try:\n",
    "        resp = oai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"user\",\"content\":ENTITY_PROMPT.format(text=text[:4000])}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "        data = json.loads(re.sub(r\"```json|```\", \"\", raw))\n",
    "        return data.get(\"entities\", [])\n",
    "    except Exception:\n",
    "        # robust regex fallback\n",
    "        ents = []\n",
    "        if re.search(r\"\\bforeign transaction\\b|\\bfx fee\\b|\\b3%\\b\", text, re.I):\n",
    "            ents.append({\"name\":\"Foreign Transaction Fee\",\"type\":\"Fee\"})\n",
    "        if re.search(r\"\\bdispute\\b|\\b60 days?\\b\", text, re.I):\n",
    "            ents.append({\"name\":\"Dispute Window\",\"type\":\"TimeWindow\"})\n",
    "        if re.search(r\"\\brewards?\\b|\\bpoints\\b|\\b1\\.5x\\b\", text, re.I):\n",
    "            ents.append({\"name\":\"Rewards\",\"type\":\"Reward\"})\n",
    "        return ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "239e978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INGEST] 6. Foreign Transaction Fees and Currency Conversion Policy.docx → 11 chunks\n",
      "[INGEST] 1. Cardholder Agreement Terms.docx → 11 chunks\n",
      "[INGEST] 8. Promotional Offers and Balance Transfers Policy .docx → 11 chunks\n",
      "[INGEST] 5. Dispute Resolution Policy.docx → 11 chunks\n",
      "[INGEST] 4. Rewards Program Terms.docx → 11 chunks\n",
      "[INGEST] 9. Privacy and Data Sharing Policy.docx → 11 chunks\n",
      "[INGEST] Card_Benefits_and_Fees.docx → 2 chunks\n",
      "[INGEST] 3. Fraud Protection and Chargeback Policy.docx → 11 chunks\n",
      "[INGEST] 10. Legal and Compliance Policy.docx → 11 chunks\n",
      "[INGEST] 2. Billing & Payment Policy.docx → 11 chunks\n",
      "[INGEST] 7. Card Benefits (Travel Insurance, Purchase Protection, and Warranty).docx → 9 chunks\n",
      "[GRAPH] Neo4j upsert complete.\n"
     ]
    }
   ],
   "source": [
    "# %% Graph ingestion\n",
    "from collections import defaultdict\n",
    "\n",
    "def graph_upsert(chunks: List['SectionChunk']):\n",
    "    # Extract entities per section\n",
    "    sec_entities: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for c in chunks:\n",
    "        ents = llm_entities(c.text)\n",
    "        sec_entities[c.section_id] = ents\n",
    "\n",
    "    if graph_store.enabled:\n",
    "        with graph_store.driver.session() as s:\n",
    "            # Basic schema\n",
    "            s.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.doc_id IS UNIQUE;\")\n",
    "            s.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Section) REQUIRE s.section_id IS UNIQUE;\")\n",
    "            s.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE;\")\n",
    "            # Upsert Docs & Sections\n",
    "            for c in chunks:\n",
    "                s.run(\"MERGE (d:Document {doc_id:$doc})\", {\"doc\": c.doc_id})\n",
    "                s.run(\"\"\"\n",
    "                    MERGE (sc:Section {section_id:$sid})\n",
    "                    SET sc.heading=$h\n",
    "                    WITH sc\n",
    "                    MATCH (d:Document {doc_id:$doc})\n",
    "                    MERGE (d)-[:HAS_SECTION]->(sc)\n",
    "                \"\"\", {\"sid\": c.section_id, \"h\": c.heading, \"doc\": c.doc_id})\n",
    "            # FOLLOWS (sequential)\n",
    "            chunks_sorted = sorted(chunks, key=lambda x: (x.doc_id, x.order))\n",
    "            for a, b in zip(chunks_sorted, chunks_sorted[1:]):\n",
    "                if a.doc_id == b.doc_id:\n",
    "                    s.run(\"\"\"\n",
    "                        MATCH (sa:Section {section_id:$a}),(sb:Section {section_id:$b})\n",
    "                        MERGE (sa)-[:FOLLOWS]->(sb)\n",
    "                    \"\"\", {\"a\": a.section_id, \"b\": b.section_id})\n",
    "            # Entities + MENTIONS\n",
    "            for sid, ents in sec_entities.items():\n",
    "                for ent in ents:\n",
    "                    s.run(\"MERGE (e:Entity {name:$n}) SET e.type=$t\", {\"n\": ent[\"name\"], \"t\": ent.get(\"type\",\"Other\")})\n",
    "                    s.run(\"\"\"\n",
    "                        MATCH (s:Section {section_id:$sid}),(e:Entity {name:$n})\n",
    "                        MERGE (s)-[:MENTIONS]->(e)\n",
    "                    \"\"\", {\"sid\": sid, \"n\": ent[\"name\"]})\n",
    "        print(\"[GRAPH] Neo4j upsert complete.\")\n",
    "    else:\n",
    "        G = graph_store.G\n",
    "        for c in chunks:\n",
    "            G.add_node(c.doc_id, label=\"Document\")\n",
    "            G.add_node(c.section_id, label=\"Section\", heading=c.heading)\n",
    "            G.add_edge(c.doc_id, c.section_id, label=\"HAS_SECTION\")\n",
    "        chunks_sorted = sorted(chunks, key=lambda x: (x.doc_id, x.order))\n",
    "        for a, b in zip(chunks_sorted, chunks_sorted[1:]):\n",
    "            if a.doc_id == b.doc_id:\n",
    "                G.add_edge(a.section_id, b.section_id, label=\"FOLLOWS\")\n",
    "        for c in chunks:\n",
    "            for ent in sec_entities[c.section_id]:\n",
    "                eid = f\"ENT::{ent['name']}\"\n",
    "                G.add_node(eid, label=\"Entity\", type=ent.get(\"type\",\"Other\"))\n",
    "                G.add_edge(c.section_id, eid, label=\"MENTIONS\")\n",
    "        print(f\"[GRAPH] In-memory graph nodes={G.number_of_nodes()} edges={G.number_of_edges()}\")\n",
    "\n",
    "# Call after your (re)indexing step has produced `chunks`\n",
    "# If you keep only `docs`, regenerate the current set of `chunks` for graph:\n",
    "def rebuild_chunks_from_folder(folder: str) -> List['SectionChunk']:\n",
    "    all_chunks: List['SectionChunk'] = []\n",
    "    for name in os.listdir(folder):\n",
    "        if name.lower().endswith(\".docx\"):\n",
    "            all_chunks += parse_docx_heading_aware(os.path.join(folder, name))\n",
    "    return all_chunks\n",
    "\n",
    "# Upsert now\n",
    "_chunks_for_graph = rebuild_chunks_from_folder(DOCS_PATH)\n",
    "graph_upsert(_chunks_for_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c546bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Graph expansion utilities\n",
    "def graph_expand_sections(section_ids: List[str], hops: int = 1) -> List[str]:\n",
    "    if not section_ids:\n",
    "        return section_ids\n",
    "    if graph_store.enabled:\n",
    "        with graph_store.driver.session() as s:\n",
    "            expanded = set(section_ids)\n",
    "            frontier = set(section_ids)\n",
    "            for _ in range(hops):\n",
    "                next_frontier = set()\n",
    "                for sid in list(frontier):\n",
    "                    res = s.run(\"\"\"\n",
    "                        MATCH (s:Section {section_id:$sid})\n",
    "                        OPTIONAL MATCH (s)-[:FOLLOWS|HAS_SECTION|MENTIONS]->(x)\n",
    "                        OPTIONAL MATCH (x)-[:FOLLOWS|HAS_SECTION|MENTIONS]->(y)\n",
    "                        WITH COLLECT(DISTINCT x) + COLLECT(DISTINCT y) AS n\n",
    "                        UNWIND n AS z\n",
    "                        WITH DISTINCT z\n",
    "                        WHERE z:Section\n",
    "                        RETURN z.section_id as sid\n",
    "                    \"\"\", {\"sid\": sid})\n",
    "                    for r in res:\n",
    "                        if r[\"sid\"] not in expanded:\n",
    "                            next_frontier.add(r[\"sid\"])\n",
    "                expanded |= next_frontier\n",
    "                frontier = next_frontier\n",
    "            return list(expanded)\n",
    "    else:\n",
    "        G = graph_store.G\n",
    "        expanded = set(section_ids)\n",
    "        frontier = set(section_ids)\n",
    "        for _ in range(hops):\n",
    "            nxt = set()\n",
    "            for sid in list(frontier):\n",
    "                for u, v, data in G.out_edges(sid, data=True):\n",
    "                    if data.get(\"label\") in {\"FOLLOWS\",\"HAS_SECTION\",\"MENTIONS\"} and G.nodes[v].get(\"label\")==\"Section\":\n",
    "                        nxt.add(v)\n",
    "                for u, v, data in G.in_edges(sid, data=True):\n",
    "                    if data.get(\"label\") in {\"FOLLOWS\",\"HAS_SECTION\",\"MENTIONS\"} and G.nodes[u].get(\"label\")==\"Section\":\n",
    "                        nxt.add(u)\n",
    "            expanded |= nxt\n",
    "            frontier = nxt\n",
    "        return list(expanded)\n",
    "\n",
    "# Hybrid + Graph expansion: replace your hybrid_retrieve with this wrapper\n",
    "def hybrid_retrieve_graph(query: str, k_dense: int=5, k_kw: int=5, graph_hops: int=1):\n",
    "    base = hybrid_retrieve(query, k_dense=k_dense, k_kw=k_kw)  # uses FAISS+BM25\n",
    "    seed_sids = [d.metadata[\"section_id\"] for d in base]\n",
    "    expanded_sids = graph_expand_sections(seed_sids, hops=graph_hops)\n",
    "    # materialize expanded sections back to Documents\n",
    "    sid_set = set(expanded_sids)\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for d in docs:\n",
    "        sid = d.metadata[\"section_id\"]\n",
    "        if sid in sid_set and sid not in seen:\n",
    "            out.append(d); seen.add(sid)\n",
    "    print(f\"[GRAPH-RETRIEVE] seeds={len(seed_sids)} expanded={len(expanded_sids)} final={len(out)}\")\n",
    "    # keep it tight for synthesis\n",
    "    return out[: max(k_dense, k_kw) + 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9c892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Synthesis (grounded)\n",
    "SYSTEM = \"\"\"You are a customer-support QA assistant.\n",
    "- Ground all answers ONLY on provided CONTEXT.\n",
    "- If insufficient, say \"needs clarification\" and ask a precise follow-up.\n",
    "- Output BOTH: (1) concise natural-language answer with [doc_id:section_id] citations, and (2) a strict JSON block matching the schema.\n",
    "\"\"\"\n",
    "\n",
    "JSON_SCHEMA = \"\"\"\n",
    "{\n",
    "  \"answer\": \"string\",\n",
    "  \"support\": [{\"doc_id\":\"string\",\"section\":\"string\",\"snippet\":\"string\",\"score\": 0.0}],\n",
    "  \"entities\": [{\"name\":\"string\",\"type\":\"string\"}],\n",
    "  \"disposition\": \"final|needs_clarification\",\n",
    "  \"confidence\": 0.0,\n",
    "  \"recommended_next_steps\": [\"string\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "memory: List[Dict[str, Any]] = []\n",
    "\n",
    "def summarize_history(history: List[Dict[str, Any]]) -> str:\n",
    "    if not history: return \"\"\n",
    "    last = history[-1]\n",
    "    return f\"Prev Q: {last.get('user','')}\\nPrev A: {last.get('answer','')[:400]}\"\n",
    "\n",
    "def remember_salient_ids(retrieved_docs: List[Any]) -> List[str]:\n",
    "    return list({d.metadata[\"section_id\"] for d in retrieved_docs})\n",
    "\n",
    "def synthesize_answer(user_query: str, retrieved_docs: List[Any]) -> Tuple[str, Dict[str, Any]]:\n",
    "    context_blocks, support = [], []\n",
    "    for d in retrieved_docs:\n",
    "        sid = d.metadata[\"section_id\"]; docid = d.metadata[\"doc_id\"]\n",
    "        context_blocks.append(f\"[{docid}:{sid}] {d.page_content}\")\n",
    "        support.append({\"doc_id\": docid, \"section\": sid, \"snippet\": d.page_content[:400], \"score\": 0.0})\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_blocks) if context_blocks else \"(no context)\"\n",
    "    hist_summary = summarize_history(memory)\n",
    "\n",
    "    prompt = f\"\"\"{SYSTEM}\n",
    "JSON schema:\n",
    "{JSON_SCHEMA}\n",
    "\n",
    "HISTORY:\n",
    "{hist_summary}\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "USER QUESTION:\n",
    "{user_query}\n",
    "\n",
    "Respond with:\n",
    "1) 2-4 sentence answer with [doc_id:section_id] citations.\n",
    "2) A JSON object strictly following the schema.\n",
    "\"\"\"\n",
    "    resp = oai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"system\",\"content\":\"You are precise and follow schemas.\"},\n",
    "                  {\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    txt = resp.choices[0].message.content\n",
    "    first = txt.find(\"{\")\n",
    "    if first == -1:\n",
    "        nat = txt.strip()\n",
    "        js = {\"answer\": nat, \"support\": support, \"entities\": [], \"disposition\":\"needs_clarification\", \"confidence\":0.3, \"recommended_next_steps\":[]}\n",
    "    else:\n",
    "        nat = txt[:first].strip()\n",
    "        js_raw = txt[first:].strip()\n",
    "        js_clean = re.sub(r\"```json|```\", \"\", js_raw)\n",
    "        try:\n",
    "            js = json.loads(js_clean)\n",
    "            if not js.get(\"support\"): js[\"support\"] = support\n",
    "        except Exception:\n",
    "            js = {\"answer\": nat, \"support\": support, \"entities\": [], \"disposition\":\"final\", \"confidence\":0.6, \"recommended_next_steps\":[]}\n",
    "\n",
    "    print(\"[SYNTH] Answer:\")\n",
    "    print(nat)\n",
    "    return nat, js\n",
    "\n",
    "def answer_query(query: str, graph_hops: int=1):\n",
    "    retrieved = hybrid_retrieve_graph(query, k_dense=5, k_kw=5, graph_hops=graph_hops)\n",
    "    nat, js = synthesize_answer(query, retrieved)\n",
    "    memory.append({\"user\": query, \"answer\": nat, \"salient_ids\": remember_salient_ids(retrieved)})\n",
    "    return nat, js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecaf512a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: python-multipart\n",
      "Successfully installed python-multipart-0.0.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-multipart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a770e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% STT + API\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.responses import JSONResponse\n",
    "import tempfile\n",
    "import uvicorn\n",
    "\n",
    "try:\n",
    "    app  # reuse if already created\n",
    "except NameError:\n",
    "    app = FastAPI(title=\"Customer Support Graph-RAG\")\n",
    "\n",
    "def transcribe_audio(file_path: str) -> Dict[str, Any]:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        tr = oai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=f,\n",
    "            response_format=\"verbose_json\"\n",
    "        )\n",
    "    text = getattr(tr, \"text\", \"\") or \"\"\n",
    "    segs = [dict(s) for s in getattr(tr, \"segments\", [])] if hasattr(tr, \"segments\") else []\n",
    "    print(f\"[STT] text_len={len(text)} segments={len(segs)}\")\n",
    "    return {\"text\": text, \"segments\": segs}\n",
    "\n",
    "@app.post(\"/stt\")\n",
    "async def stt_endpoint(audio: UploadFile = File(...), graph_hops: int = 1):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "        tmp.write(await audio.read())\n",
    "        tmp_path = tmp.name\n",
    "    stt = transcribe_audio(tmp_path)\n",
    "    q = stt[\"text\"].strip()\n",
    "    if not q:\n",
    "        return JSONResponse({\"error\":\"empty transcription\"}, status_code=400)\n",
    "    nat, js = answer_query(q, graph_hops=graph_hops)\n",
    "    return JSONResponse({\"transcript\": q, \"answer\": nat, \"json\": js})\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat_endpoint(query: str = Form(...), graph_hops: int = 1):\n",
    "    print(f\"[CHAT] {query}\")\n",
    "    nat, js = answer_query(query, graph_hops=graph_hops)\n",
    "    return JSONResponse({\"answer\": nat, \"json\": js})\n",
    "\n",
    "# Optional: hot reindex without restart (works with your dynamic indexer)\n",
    "@app.post(\"/reindex\")\n",
    "def reindex_api(force: bool=False):\n",
    "    print(\"[API] /reindex called\")\n",
    "    reindex_policies(force=force)\n",
    "    # refresh graph too\n",
    "    _ch = rebuild_chunks_from_folder(DOCS_PATH)\n",
    "    graph_upsert(_ch)\n",
    "    return JSONResponse({\"ok\": True, \"docs\": len(docs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16db545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRIEVE] dense=5 kw=5 → fused=10\n",
      "[GRAPH-RETRIEVE] seeds=5 expanded=5 final=5\n",
      "[SYNTH] Answer:\n",
      "Yes, if you use your card abroad, you will incur a 3% foreign transaction fee on all purchases made in a foreign currency or outside your home country. This fee is calculated as 3% of the total transaction amount and will be added to your statement in your home currency [6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262:6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262::sec1]. Additionally, if you make cash withdrawals from an ATM outside your home country, a $5 fixed fee will apply along with the 3% foreign transaction fee [6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262:6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262::sec3].\n",
      "\n",
      "```json\n",
      "\n",
      "--- NAT ---\n",
      " Yes, if you use your card abroad, you will incur a 3% foreign transaction fee on all purchases made in a foreign currency or outside your home country. This fee is calculated as 3% of the total transaction amount and will be added to your statement in your home currency [6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262:6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262::sec1]. Additionally, if you make cash withdrawals from an ATM outside your home country, a $5 fixed fee will apply along with the 3% foreign transaction fee [6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262:6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262::sec3].\n",
      "\n",
      "```json\n",
      "\n",
      "--- JSON ---\n",
      " {\n",
      "  \"answer\": \"Yes, if you use your card abroad, you will incur a 3% foreign transaction fee on all purchases made in a foreign currency or outside your home country. This fee is calculated as 3% of the total transaction amount and will be added to your statement in your home currency. Additionally, if you make cash withdrawals from an ATM outside your home country, a $5 fixed fee will apply along with the 3% foreign transaction fee.\",\n",
      "  \"support\": [\n",
      "    {\n",
      "      \"doc_id\": \"6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262\",\n",
      "      \"section\": \"sec1\",\n",
      "      \"snippet\": \"A 3% foreign transaction fee applies to all purchases made in a foreign currency or outside the cardholder\\u2019s home country.\",\n",
      "      \"score\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"doc_id\": \"6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262\",\n",
      "      \"section\": \"sec3\",\n",
      "      \"snippet\": \"When making cash withdrawals from an ATM outside the cardholder\\u2019s home country, a $5 fixed fee applies in additi\n",
      "[RETRIEVE] dense=5 kw=5 → fused=7\n",
      "[GRAPH-RETRIEVE] seeds=5 expanded=5 final=5\n",
      "[SYNTH] Answer:\n",
      "You must report disputes within 60 days of the transaction date appearing on the billing statement. If disputes are reported after this period, they may not be accepted by the issuer [5. Dispute Resolution Policy-e77e13de:5. Dispute Resolution Policy-e77e13de::sec2]. \n",
      "\n",
      "```json\n",
      "\n",
      "--- NAT ---\n",
      " You must report disputes within 60 days of the transaction date appearing on the billing statement. If disputes are reported after this period, they may not be accepted by the issuer [5. Dispute Resolution Policy-e77e13de:5. Dispute Resolution Policy-e77e13de::sec2]. \n",
      "\n",
      "```json\n"
     ]
    }
   ],
   "source": [
    "# %% E2E smoke tests (text)\n",
    "q1 = \"If I use my card abroad, will I be charged a fee?\"\n",
    "nat, js = answer_query(q1, graph_hops=1)\n",
    "print(\"\\n--- NAT ---\\n\", nat)\n",
    "print(\"\\n--- JSON ---\\n\", json.dumps(js, indent=2)[:1000])\n",
    "\n",
    "q2 = \"What is the dispute reporting window?\"\n",
    "nat, _ = answer_query(q2, graph_hops=1)\n",
    "print(\"\\n--- NAT ---\\n\", nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a2d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile, time, json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# --- 1) Transcribe an audio FILE with Whisper ---\n",
    "def transcribe_audio_file(path: str) -> Dict[str, Any]:\n",
    "    assert os.path.exists(path), f\"Audio file not found: {path}\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        tr = oai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=f,\n",
    "            response_format=\"verbose_json\"\n",
    "        )\n",
    "    text = getattr(tr, \"text\", \"\") or \"\"\n",
    "    segs = [dict(s) for s in getattr(tr, \"segments\", [])] if hasattr(tr, \"segments\") else []\n",
    "    print(f\"[STT:file] text_len={len(text)} segments={len(segs)}\")\n",
    "    return {\"text\": text, \"segments\": segs}\n",
    "\n",
    "# --- 2) Capture mic → WAV → transcribe ---\n",
    "def record_microphone(seconds: float = 8.0, samplerate: int = 16000) -> str:\n",
    "    \"\"\"\n",
    "    Records from the default microphone for `seconds` and writes a temp WAV.\n",
    "    Returns path to the WAV file.\n",
    "    \"\"\"\n",
    "    import sounddevice as sd\n",
    "    from scipy.io.wavfile import write as wav_write\n",
    "\n",
    "    print(f\"[REC] Recording {seconds}s @ {samplerate}Hz… Speak now.\")\n",
    "    audio = sd.rec(int(seconds * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    wav_write(tmp.name, samplerate, audio)\n",
    "    print(f\"[REC] Saved: {tmp.name}\")\n",
    "    return tmp.name\n",
    "\n",
    "def stt_to_answer_from_file(path: str, graph_hops: int = 1) -> Dict[str, Any]:\n",
    "    stt = transcribe_audio_file(path)\n",
    "    q = stt[\"text\"].strip()\n",
    "    if not q:\n",
    "        print(\"[STT] Empty transcript.\")\n",
    "        return {\"transcript\": \"\", \"answer\": \"\", \"json\": {}}\n",
    "    print(f\"[PIPELINE] User said: {q}\")\n",
    "    nat, js = answer_query(q, graph_hops=graph_hops)\n",
    "    return {\"transcript\": q, \"answer\": nat, \"json\": js}\n",
    "\n",
    "def stt_to_answer_from_mic(seconds: float = 8.0, graph_hops: int = 1) -> Dict[str, Any]:\n",
    "    wav_path = record_microphone(seconds=seconds)\n",
    "    try:\n",
    "        return stt_to_answer_from_file(wav_path, graph_hops=graph_hops)\n",
    "    finally:\n",
    "        try: os.remove(wav_path)\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4764e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.2-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (1.13.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/anaconda3/lib/python3.12/site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Downloading sounddevice-0.5.2-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl (108 kB)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2a83cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REC] Recording 6s @ 16000Hz… Speak now.\n",
      "[REC] Saved: /var/folders/6v/dnjkft556qngc04m0chvx21h0000gn/T/tmp8gogcbwh.wav\n",
      "[STT:file] text_len=58 segments=1\n",
      "[PIPELINE] User said: Does the foreign transactions fee apply to the cardholder?\n",
      "[RETRIEVE] dense=5 kw=5 → fused=6\n",
      "[GRAPH-RETRIEVE] seeds=5 expanded=5 final=5\n",
      "[SYNTH] Answer:\n",
      "Yes, a foreign transaction fee of 3% applies to all purchases made in a foreign currency or outside of the cardholder’s home country. This fee is calculated as 3% of the total transaction amount and will be added to the cardholder’s statement in their home currency [6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262:6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262::sec1].\n",
      "\n",
      "```json\n",
      "\n",
      "[TRANSCRIPT]\n",
      " Does the foreign transactions fee apply to the cardholder?\n",
      "\n",
      "[ANSWER]\n",
      " Yes, a foreign transaction fee of 3% applies to all purchases made in a foreign currency or outside of the cardholder’s home country. This fee is calculated as 3% of the total transaction amount and will be added to the cardholder’s statement in their home currency [6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262:6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262::sec1].\n",
      "\n",
      "```json\n",
      "\n",
      "[JSON]\n",
      " {\n",
      "  \"answer\": \"Yes, a foreign transaction fee of 3% applies to all purchases made in a foreign currency or outside of the cardholder\\u2019s home country. This fee is calculated as 3% of the total transaction amount and will be added to the cardholder\\u2019s statement in their home currency.\",\n",
      "  \"support\": [\n",
      "    {\n",
      "      \"doc_id\": \"6. Foreign Transaction Fees and Currency Conversion Policy-35ea3262\",\n",
      "      \"section\": \"sec1\",\n",
      "      \"snippet\": \"A 3% foreign transaction fee applies to all purchases made in a foreign currency or outside the cardholder\\u2019s home country.\",\n",
      "      \"score\": 1.0\n",
      "    }\n",
      "  ],\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"foreign transaction fee\",\n",
      "      \"type\": \"fee\"\n",
      "    }\n",
      "  ],\n",
      "  \"disposition\": \"final\",\n",
      "  \"confidence\": 1.0,\n",
      "  \"recommended_next_steps\": [\n",
      "    \"Review your cardholder agreement for specific details on fees.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Mic demo: speak a question like \"Do I pay a foreign transaction fee abroad?\" ---\n",
    "result = stt_to_answer_from_mic(seconds=6, graph_hops=1)\n",
    "print(\"\\n[TRANSCRIPT]\\n\", result[\"transcript\"])\n",
    "print(\"\\n[ANSWER]\\n\", result[\"answer\"])\n",
    "print(\"\\n[JSON]\\n\", json.dumps(result[\"json\"], indent=2)[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd071f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
